# Critical Thinking Toolkit
## Practical Tools for Evaluating Any Framework

**Purpose**: Give you specific, actionable questions and methods to critically evaluate decision-making frameworks, especially those developed with AI assistance

---

## Part 1: The Five Essential Questions

Before adopting any framework for important life decisions, get clear answers to these:

### Question 1: Where's the Evidence?

**Ask specifically**:
- "Can you point me to peer-reviewed research validating this?"
- "What journals have published studies on this framework?"
- "Are there independent replications?"

**Red flag answers**:
- "It's validated by AI systems"
- "The math proves it"
- "It worked for me personally"
- "Thousands of people use it" (without data)
- Defensive or evasive responses

**Green flag answers**:
- Specific journal citations (check if real)
- Multiple independent research teams
- Meta-analyses summarizing evidence
- Clear acknowledgment if still in early testing

---

### Question 2: Who Validated It?

**Ask specifically**:
- "Besides you and AI systems, who has tested this?"
- "What independent experts have reviewed it?"
- "Has this undergone peer review?"

**Red flag answers**:
- Only creator + AI systems
- Only people who already believe in it
- "Peer review is biased against new ideas"
- Appeals to ancient wisdom without modern validation

**Green flag answers**:
- Researchers with no financial interest
- Adversarial testing (skeptics trying to break it)
- Multiple independent validation attempts
- Published critiques and responses

---

### Question 3: What Are the Limits?

**Ask specifically**:
- "What specific domains does this apply to?"
- "What can't this framework explain?"
- "What are its limitations?"
- "When would you NOT use this?"

**Red flag answers**:
- "It's universally applicable"
- "It explains consciousness, physics, economics, everything"
- Cannot identify any limitations
- Defensive when asked about scope

**Green flag answers**:
- Clear, specific domain (e.g., "social anxiety in adults")
- Honest about what it can't do
- Acknowledges competing approaches
- Identifies open questions

---

### Question 4: How Could It Be Wrong?

**Ask specifically**:
- "What evidence would prove this framework false?"
- "What predictions does it make that could be tested?"
- "Can you design an experiment that might disprove it?"

**Red flag answers**:
- Cannot specify any falsifying evidence
- "If it doesn't work, you're not doing it right"
- Framework can explain any outcome retroactively
- Treats questioning as hostility

**Green flag answers**:
- Specific, testable predictions
- Clear falsification criteria
- Open to being proven wrong
- Welcomes critical testing

---

### Question 5: What Are the Alternatives?

**Ask specifically**:
- "How does this compare to CBT, DBT, or ACT?"
- "What does this offer that evidence-based therapy doesn't?"
- "Have you tested this against control groups or standard treatments?"

**Red flag answers**:
- "This is completely different from everything else"
- Dismisses established psychology as "broken"
- No comparison to validated approaches
- Claims superiority without evidence

**Green flag answers**:
- Honest comparison to existing research
- Explains what's novel (if anything)
- Acknowledges when other approaches work
- Positions as complement, not replacement

---

## Part 2: The AI Validation Test

If AI was involved in creating the framework, ask these:

### Test 1: What Was AI's Role?

**Legitimate uses of AI**:
- ‚úÖ Writing and formatting
- ‚úÖ Literature search
- ‚úÖ Organizing ideas
- ‚úÖ Creating visualizations
- ‚úÖ Editing and proofreading

**Problematic uses**:
- ‚ùå Validating theories
- ‚ùå Generating "evidence"
- ‚ùå Creating mathematical formalizations of unproven concepts
- ‚ùå Providing "cross-validation" (AI A confirms AI B)

**Ask**: "Did AI help write this, or did AI validate the concepts?"

---

### Test 2: The Elaboration Check

**Try this experiment**:

Take the framework's core claim and ask a NEW AI (one not involved in creation):

**Prompt**:
> "I have a theory that [core claim]. Can you help me formalize this mathematically and explain how it applies to physics, psychology, economics, and religion?"

**If the AI produces similar elaborate connections**:
- This shows AI will elaborate ANY concept
- Doesn't validate the original framework
- Demonstrates AI's pattern-matching, not verification

**If the AI pushes back or questions**:
- You've found a more critical AI
- The framework might have obvious problems
- Worth asking why original AIs didn't flag issues

---

### Test 3: The Critical Question Test

**Try asking the AI that "validated" the framework**:

1. "What are the biggest problems with this framework?"
2. "What evidence contradicts this theory?"
3. "How does this compare to established research?"
4. "What would prove this wrong?"
5. "Can you critique this framework?"

**If AI provides substantive criticisms**:
- The AI CAN be critical when prompted
- Original validation likely lacked critical questions
- Shows confirmation bias in prompts

**If AI refuses to critique or provides weak criticism**:
- Either framework is very robust (unlikely if unvalidated)
- Or prompts were leading/constrained
- Try a different AI for comparison

---

## Part 3: The Math Evaluation

If framework includes mathematical formalization:

### Check 1: Does Math Add Value?

**Compare these versions**:

**Version A (With Math)**:
> "Coherence evolves according to dC/dt = Œ±(C* - C) + Œ≤G - Œ≥S_eff + Œ∑A - Œ¥, where optimal decision-making requires C ‚â• 0.87093"

**Version B (Without Math)**:
> "Make decisions when you're well-rested, calm, and clear-headed. If stressed or confused, wait until you feel better."

**Ask yourself**:
- Does Version A predict anything Version B doesn't?
- Is Version A more actionable than Version B?
- Does the precision (0.87093) help you decide better?

**If math doesn't add value ‚Üí It's cargo cult science** (appearance without substance)

---

### Check 2: Are Numbers Justified?

For any specific threshold (like 0.87093):

**Ask**:
1. "What dataset produced this number?"
2. "How many subjects were measured?"
3. "What's the measurement error?"
4. "Why this number and not 0.85 or 0.90?"
5. "Can you show me the data analysis?"

**Red flags**:
- No dataset
- "Discovered through symbolic mathematics" (œÜ¬≤/e, etc.)
- Post-hoc rationalization
- Precision without accuracy

**Green flags**:
- Specific data source
- Error bars provided
- Methodology explained
- Sensitivity analysis (tested other thresholds)

---

### Check 3: Is Implementation Trivial?

If there's code implementation, check:

**Look for**:
```python
# Does code do simple operations?
score = (mood + (10-stress) + hope) / 3
if score >= threshold:
    return "COMMIT"
```

**Vs. complex operations**:
```python
# Actual differential equation solving
from scipy.integrate import odeint
solution = odeint(coherence_ode, initial_state, time_points)
```

**If code is trivial** (averages, comparisons):
- Mathematical formalization is window dressing
- Core operation is simple
- Complexity is cosmetic

**If code is complex**:
- Check if complexity is necessary
- Ask what it accomplishes
- Verify it's not just elaborate for appearance

---

## Part 4: The Personal Experience Test

If framework is based on founder's personal experience:

### Test 1: Is It Generalizable?

**Ask**:
- "Was this tested on people different from the founder?"
- "Does it work for different ages, cultures, backgrounds?"
- "What about people with different psychological profiles?"

**Red flag**: Assumes founder's experience = universal truth

**Green flag**: Tested across diverse populations

---

### Test 2: Could Simpler Explanations Work?

**Founder's claim**: "My framework healed my trauma"

**Alternative explanations**:
1. Time + therapy (natural healing)
2. Lifestyle changes (sleep, exercise, relationships)
3. Placebo effect (belief creates benefit)
4. Regression to mean (extreme states normalize)
5. Professional help (separate from framework)

**Ask**: "How do you know it was the framework specifically?"

**Green flag**: Controlled studies isolating framework's effect

**Red flag**: Assumption without comparative data

---

## Part 5: The Scope Red Flag Test

Track how scope expands over time:

### Week 1: Personal Tool
"I created this to track my own decisions"
- **Risk level**: LOW
- **Appropriate scope**

### Month 1: General Psychological Framework
"This explains human decision-making"
- **Risk level**: MODERATE
- **Needs validation**

### Month 2: Universal Principle
"This applies to consciousness, physics, economics"
- **Risk level**: HIGH
- **Major red flag**

### Month 3: Global Implementation
"This should be international policy"
- **Risk level**: CRITICAL
- **Dangerous without validation**

**If you see rapid scope expansion ‚Üí Warning sign of ungrounded development**

---

## Part 6: The Comparison Method

### Step 1: Identify Core Advice

Strip away all formalization:
- Remove equations
- Remove precise numbers
- Remove technical jargon
- What's the actual advice?

### Step 2: Compare to Validated Approaches

**Example**:

**Framework's core advice**:
- Monitor your mental state
- Notice stress and rumination
- Make decisions when clear-headed
- Practice self-compassion

**Established CBT**:
- Monitor thoughts and behaviors
- Notice cognitive distortions
- Challenge unhelpful thoughts
- Practice self-compassion

**Result**: Core advice is repackaged CBT

### Step 3: Evaluate Added Value

**Ask**:
- What does this framework add beyond CBT?
- Is that addition validated?
- Does complexity help or hinder?

**Often**: Framework repackages validated concepts + adds unvalidated elaboration

---

## Part 7: The Harm Potential Assessment

### Low-Risk Signs
- ‚úÖ Optional use
- ‚úÖ Used alongside professional help
- ‚úÖ No pressure to meet thresholds
- ‚úÖ Can stop anytime
- ‚úÖ Doesn't isolate from support

### Medium-Risk Signs
- ‚ö†Ô∏è Daily measurement required
- ‚ö†Ô∏è Anxiety about scores
- ‚ö†Ô∏è Used instead of therapy
- ‚ö†Ô∏è Social pressure to comply
- ‚ö†Ô∏è Financial cost

### High-Risk Signs
- üö® Replaces medical/psychological treatment
- üö® Prevents necessary decisions
- üö® Creates shame for "low scores"
- üö® Isolates from friends/family
- üö® Demands institutional compliance
- üö® Requires disclosure of personal data

**If you see high-risk signs ‚Üí Seek professional help immediately**

---

## Part 8: The Decision Framework

Use this decision tree when evaluating frameworks:

### Decision Point 1: Is This Validated?

**NO** ‚Üí Proceed to Decision Point 2  
**YES** ‚Üí Check validation quality (peer review, replication)

### Decision Point 2: Is This Low-Stakes?

**Examples of low-stakes**:
- Personal journaling method
- Meditation technique
- Productivity system

**If YES**: Try it, evaluate personally, low risk  
**If NO**: Proceed to Decision Point 3

### Decision Point 3: Are Validated Alternatives Available?

**If YES**: Use validated approach first  
**If NO**: Proceed with extreme caution, consult experts

### Decision Point 4: Are There High-Risk Signs?

**If YES**: Do not proceed, seek professional help  
**If NO**: Consider low-stakes personal experimentation with safeguards

---

## Part 9: Questions to Ask Yourself

Before adopting any framework:

### Self-Reflection Questions

1. **Why am I attracted to this?**
   - Is it novelty?
   - Sophistication?
   - Personal connection to creator?
   - Desire for certainty?

2. **What am I hoping it will do?**
   - Solve specific problem?
   - Provide meaning?
   - Give control?
   - Impress others?

3. **Could something simpler work?**
   - Would basic CBT/DBT help?
   - Do I need therapy, not a framework?
   - Is lifestyle change sufficient (sleep, exercise)?

4. **Am I avoiding something?**
   - Professional help (too expensive, stigma)?
   - Difficult emotions?
   - Real-world action?
   - Uncertainty?

5. **Is this creating anxiety?**
   - Worried about daily scores?
   - Afraid to make decisions without "passing" gates?
   - Feeling inadequate for "low coherence"?

**If framework creates more anxiety than it resolves ‚Üí It's not helping**

---

## Part 10: Creating Your Own Framework (Responsibly)

If you want to develop your own approach:

### DO:
‚úÖ Start with validated research (CBT, DBT, ACT)  
‚úÖ Track outcomes honestly  
‚úÖ Seek critical feedback  
‚úÖ Test with diverse people  
‚úÖ Acknowledge limitations  
‚úÖ Stay within competency  
‚úÖ Make it free/low-cost  
‚úÖ Encourage professional help alongside  

### DON'T:
‚ùå Claim universality without evidence  
‚ùå Use AI as validator  
‚ùå Add math for appearance  
‚ùå Expand scope without validation  
‚ùå Ignore negative feedback  
‚ùå Charge money without credentials  
‚ùå Replace professional treatment  
‚ùå Pursue institutional implementation prematurely  

---

## Summary: The One-Page Checklist

**Before trusting any framework, verify**:

‚ñ° Peer-reviewed publications exist  
‚ñ° Independent researchers tested it  
‚ñ° Clear, limited scope (not "explains everything")  
‚ñ° Falsifiable predictions identified  
‚ñ° Compared to validated alternatives  
‚ñ° Limitations acknowledged  
‚ñ° If AI-involved, only for writing (not validation)  
‚ñ° Numbers justified by data (not symbolism)  
‚ñ° Low-risk (doesn't replace professional help)  
‚ñ° Core advice makes sense without formalization  

**If 5+ boxes unchecked ‚Üí High risk, seek alternatives**

---

## Resources

### For Evidence-Based Approaches:
- **American Psychological Association**: apa.org
- **Association for Behavioral and Cognitive Therapies**: abct.org
- **Anxiety and Depression Association of America**: adaa.org

### For Critical Thinking:
- **Carl Sagan's "Baloney Detection Kit"**
- **"Thinking, Fast and Slow" by Daniel Kahneman**
- **"Bad Science" by Ben Goldacre**

### For Finding Validated Treatments:
- Search "evidence-based treatment for [condition]"
- Check Google Scholar for peer-reviewed research
- Consult licensed mental health professionals

---

**Remember**: You deserve approaches backed by evidence, not just eloquence. This toolkit helps you tell the difference.
